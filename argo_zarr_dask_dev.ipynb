{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a9dc46",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dcdb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from functools import partial\n",
    "import time\n",
    "import numcodecs\n",
    "import zarr\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ded6fd",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a97417b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "\n",
    "def read_dataset_inmemory(s3_path: str) -> xr.Dataset:\n",
    "    \"\"\"Read a NetCDF as an XArray using in-memory data\"\"\"\n",
    "    try:\n",
    "        with io.BytesIO() as inmemoryfile:\n",
    "            # Use boto to download a file to memory\n",
    "            s3 = boto3.client(\"s3\")\n",
    "            bucket, key = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            s3.download_fileobj(bucket, key, inmemoryfile)\n",
    "            inmemoryfile.seek(0)\n",
    "\n",
    "            return xr.open_dataset(inmemoryfile)\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to open the file with error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a4b5f",
   "metadata": {},
   "source": [
    "### Argo Processor\n",
    "\n",
    "**Source notebook**\n",
    "https://medium.com/@nicolasmortimer/argo-floats-zarr-and-pangeo-d74fc6d4ce35\n",
    "\n",
    "*Written by Nicolas Mortimer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61b652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types ={'CONFIG_MISSION_NUMBER':'float32','CYCLE_NUMBER':'float32','DATA_CENTRE':'|U2','DATA_MODE':'|U1',\n",
    "             'DATA_STATE_INDICATOR':'|U4','DC_REFERENCE':'|U32','DIRECTION':'|U1','FIRMWARE_VERSION':'|U32',\n",
    "             'FLOAT_SERIAL_NO':'|U32','JULD':'float32','JULD_LOCATION':'float32','JULD_QC':'|U1','LATITUDE':'float32',\n",
    "             'LONGITUDE':'float32','PI_NAME':'|U64','PLATFORM_NUMBER':'|U8','PLATFORM_TYPE':'|U32','POSITIONING_SYSTEM':'|U8',\n",
    "             'POSITION_QC':'|U1','PRES':'float32','PRES_ADJUSTED':'float32','PRES_ADJUSTED_ERROR':'float32',\n",
    "             'PRES_ADJUSTED_QC':'|U1','PRES_QC':'|U1','PROFILE_PRES_QC':'|U1','PROFILE_PSAL_QC':'|U1','PROFILE_TEMP_QC':'|U1',\n",
    "             'PROJECT_NAME':'|U64','PSAL':'float32','PSAL_ADJUSTED':'float32','PSAL_ADJUSTED_ERROR':'float32',\n",
    "             'PSAL_ADJUSTED_QC':'|U1','PSAL_QC':'|U1','TEMP':'float32','TEMP_ADJUSTED':'float32','TEMP_ADJUSTED_ERROR':'float32',\n",
    "             'TEMP_ADJUSTED_QC':'|U1','TEMP_QC':'|U1','VERTICAL_SAMPLING_SCHEME':'|U256','WMO_INST_TYPE':'|U4'}\n",
    "\n",
    "data_levels =['PRES','PRES_ADJUSTED','PRES_ADJUSTED_ERROR','PRES_ADJUSTED_QC','PRES_QC','PSAL','PSAL_ADJUSTED',\n",
    "              'PSAL_ADJUSTED_ERROR','PSAL_ADJUSTED_QC','PSAL_QC','TEMP','TEMP_ADJUSTED','TEMP_ADJUSTED_ERROR',\n",
    "              'TEMP_ADJUSTED_QC','TEMP_QC']\n",
    "\n",
    "def process_mf(dsinput,levels,data_types=data_types,data_levels=data_levels):\n",
    "    ds = xr.Dataset()\n",
    "    dims =('N_PROF','N_LEVELS')\n",
    "    # The number of profiles is indicated by the N_PROF dimension\n",
    "    # The number of pressure levels is indicated by the N_LEVELS dimension\n",
    "    pading =xr.DataArray(np.ones((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))) *np.nan,dims=dims)\n",
    "    pad_qc = xr.DataArray(np.chararray((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))),dims=dims)\n",
    "    pad_qc[:] = b' '\n",
    "    for varname in data_types.keys():\n",
    "        if varname in dsinput.data_vars:\n",
    "            da = dsinput[varname]\n",
    "            if 'N_LEVELS' in da.dims:   \n",
    "                if varname in dsinput.data_vars:\n",
    "                    if varname.endswith('QC'):\n",
    "                        da = xr.concat([dsinput[varname],pad_qc],dim='N_LEVELS').astype(data_types[varname])\n",
    "                    else:\n",
    "                        da = xr.concat([dsinput[varname],pading],dim='N_LEVELS').astype(data_types[varname])\n",
    "            else:\n",
    "                da = dsinput[varname].astype(data_types[varname])\n",
    "        else:\n",
    "            if varname in data_levels:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones((len(dsinput.N_PROF),levels), dtype='float32')*np.nan , name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF),levels))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "            else:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones(len(dsinput.N_PROF), dtype=\"float32\")*np.nan , name=varname, dims=['N_PROF'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF)))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF'])\n",
    "        if not ('HISTORY' in varname) and ('N_CALIB' not in da.dims) and ('N_PARAM' not in da.dims) and  ('N_PROF' in da.dims):\n",
    "                ds[varname]= da\n",
    "    return ds.chunk({'N_LEVELS':levels})\n",
    "   \n",
    "preproc = partial(process_mf,levels=3000)\n",
    "\n",
    "@dask.delayed\n",
    "def process_float(s3_uri):\n",
    "    file = read_dataset_inmemory(s3_uri)\n",
    "    data = preproc(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefc67e",
   "metadata": {},
   "source": [
    "### Generate dataset and export to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e8038b-6f89-49a0-8838-ecc98ff2ca7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 344 ms, sys: 36.8 ms, total: 380 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "input_paths = []\n",
    "# glob_result = s3.glob('s3://imos-data/IMOS/Argo/dac/csiro/7900324/profiles/*.nc')\n",
    "glob_result = s3.glob('s3://imos-data/IMOS/Argo/dac/csiro/7900324/profiles/*.nc')\n",
    "input_paths.extend(['s3://' + path for path in glob_result])\n",
    "store_path = 's3://imos-data-pixeldrill/vhnguyen/emr/argo/temp/temp.zarr'\n",
    "# store_path = './emr/argo.zarr/'\n",
    "emr_dns = 'localhost'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c13b6-7d2f-4a5d-b98a-6db19f85c4dd",
   "metadata": {},
   "source": [
    "### Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76070b",
   "metadata": {},
   "source": [
    "#### EMR cluster dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd6e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask_yarn import YarnCluster\n",
    "# from dask.distributed import Client\n",
    "\n",
    "# # Create a cluster\n",
    "# cluster = YarnCluster()\n",
    "# # Scale up to x workers\n",
    "# cluster.scale(2)\n",
    "\n",
    "# # Connect to the cluster\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1add1bd",
   "metadata": {},
   "source": [
    "#### Local dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449e170-ccff-42c0-a09f-5a83098c250d",
   "metadata": {},
   "source": [
    "### Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca3ae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Initilising futures ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 273/273 [03:11<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** Processing Zarr **************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate xarray Dataset and DataArray objects, got <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m store\u001b[38;5;241m=\u001b[39m s3fs\u001b[38;5;241m.\u001b[39mS3Map(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, s3\u001b[38;5;241m=\u001b[39ms3, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# zarrs = [future.result() for future in futures]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzarrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN_PROF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mminimal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverride\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverride\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m synchronizer \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mProcessSynchronizer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/argodask2.sync\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m z \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mto_zarr(store, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, synchronizer\u001b[38;5;241m=\u001b[39msynchronizer, compute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xarray/core/concat.py:234\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     f \u001b[38;5;241m=\u001b[39m _dataset_concat\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\n\u001b[1;32m    239\u001b[0m     objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs\n\u001b[1;32m    240\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate xarray Dataset and DataArray objects, got <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "with dask.config.set({'temporary_directory': '/home/vietnguyen/dask/'}):\n",
    "    # set up cluster and workers\n",
    "    cluster = LocalCluster(n_workers=3, memory_limit='4GB', processes=True, threads_per_worker=2, dashboard_address=':0', ip=emr_dns)\n",
    "    client = Client(address=cluster.scheduler_address)\n",
    "\n",
    "print(f'http://{emr_dns}'+':{port}/status'.format(port=client.scheduler_info().get('services').get('dashboard')))\n",
    "\n",
    "start_time = time.time()\n",
    "# futures = []\n",
    "zarrs = []\n",
    "\n",
    "print('************ Initilising futures ************')\n",
    "for i in tqdm(range(len(input_paths))):\n",
    "    # futures.append(client.submit(process_float, input_paths[i], retries=10))\n",
    "    zarrs.append(dask.compute(process_float(input_paths[i])))\n",
    "\n",
    "print('************** Processing Zarr **************')\n",
    "\n",
    "store= s3fs.S3Map(root=f'{store_path}', s3=s3, check=False)\n",
    "# zarrs = [future.result() for future in futures]\n",
    "ds = xr.concat(zarrs,dim='N_PROF',coords='minimal',compat='override',combine_attrs='override', fill_value='')\n",
    "\n",
    "synchronizer = zarr.ProcessSynchronizer(f'{store_path}/argodask2.sync')\n",
    "\n",
    "z = ds.to_zarr(store, mode='w', synchronizer=synchronizer, compute=False)\n",
    "z.compute()\n",
    "\n",
    "print('*********************************************')\n",
    "print(\"---------- Total: %.2f seconds ----------\" % (time.time() - start_time))\n",
    "print('*********************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57facc6e",
   "metadata": {},
   "source": [
    "### Open Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = xr.open_zarr(store_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def np_dt64_to_dt(in_datetime: np.datetime64) -> str:\n",
    "    \"\"\"Convert numpy datetime64 to datetime\"\"\"\n",
    "    dt = datetime.fromtimestamp(in_datetime.astype(int) / 1e9)\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "nprof = round(len(input_paths) / 2) #Selected profile\n",
    "plt.scatter(data.PSAL_ADJUSTED[nprof], data.TEMP_ADJUSTED[nprof], c=data.PRES_ADJUSTED[nprof], cmap='viridis_r')\n",
    "plt.xlabel('Salinity');\n",
    "plt.ylabel('Temperature (°C)')\n",
    "\n",
    "cbh = plt.colorbar();\n",
    "cbh.set_label('Pressure (dbar)')\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Argo Float #%d on %s' % (data.PLATFORM_NUMBER[nprof].values, np_dt64_to_dt(data.JULD[nprof].values)), fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcf1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a73ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
